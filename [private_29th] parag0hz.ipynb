{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98657c42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ultralytics import YOLO\n",
    "from pdf2image import convert_from_path\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import pytesseract\n",
    "\n",
    "# --- 환경 설정 ---\n",
    "# GPU 사용 설정 (평가 서버에 GPU가 있으므로 'cuda'로 설정됩니다)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 모델 로딩 (전부 로컬 경로에서 로드) ---\n",
    "\n",
    "# 1. YOLO 레이아웃 모델 로딩 (제출 폴더 내 'model' 폴더)\n",
    "layout_model_path = \"./model/yolov12l-doclaynet.pt\"\n",
    "layout_model = YOLO(layout_model_path)\n",
    "print(f\"Layout model loaded from: {layout_model_path}\")\n",
    "\n",
    "# 2. Qwen2-VL OCR 모델 로딩 (제출 폴더 내 'models' 폴더)\n",
    "# 이 경로는 submit.zip 파일 내부 구조와 일치해야 합니다.\n",
    "local_vlm_path = \"./model/qwen2.5-vl-3b-instruct\"\n",
    "print(f\"Loading VLM-OCR model from local files: {local_vlm_path}...\")\n",
    "\n",
    "try:\n",
    "    # 먼저 processor만 로딩 시도\n",
    "    print(\"Loading processor...\")\n",
    "    ocr_processor = AutoProcessor.from_pretrained(local_vlm_path, trust_remote_code=True)\n",
    "    print(\"Processor loaded successfully.\")\n",
    "    \n",
    "    # 모델 로딩 시도 - AutoModelForImageTextToText 사용\n",
    "    print(\"Loading model...\")\n",
    "    \n",
    "    # Qwen2-VL-2B-Instruct 모델 로딩\n",
    "    ocr_model = AutoModelForImageTextToText.from_pretrained(\n",
    "        local_vlm_path, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # GPU로 이동\n",
    "    if device.type == 'cuda':\n",
    "        ocr_model = ocr_model.to(device)\n",
    "        \n",
    "    print(\"VLM-OCR model loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading VLM-OCR model: {e}\")\n",
    "    print(\"Falling back to basic OCR...\")\n",
    "    ocr_processor = None\n",
    "    ocr_model = None\n",
    "\n",
    "# --- 전역 변수 및 설정 ---\n",
    "# 클래스 이름 매핑\n",
    "LABEL_MAP = {\n",
    "    'Text': 'text',\n",
    "    'Title': 'title',\n",
    "    'Section-header': 'subtitle',\n",
    "    'Formula': 'equation',\n",
    "    'Table': 'table',\n",
    "    'Picture': 'image'\n",
    "}\n",
    "\n",
    "# --- 함수 정의 ---\n",
    "\n",
    "def convert_to_images(input_path, temp_dir, dpi=200):\n",
    "    ext = Path(input_path).suffix.lower()\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        return convert_from_path(input_path, dpi=dpi, output_folder=temp_dir, fmt=\"png\")\n",
    "    elif ext == \".pptx\":\n",
    "        # Convert pptx to pdf first\n",
    "        subprocess.run([\n",
    "            \"libreoffice\", \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", temp_dir, input_path\n",
    "        ], check=True)\n",
    "        pdf_path = os.path.join(temp_dir, Path(input_path).with_suffix(\".pdf\").name)\n",
    "        return convert_from_path(pdf_path, dpi=dpi, output_folder=temp_dir, fmt=\"png\")\n",
    "    elif ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        return [Image.open(input_path).convert(\"RGB\")]\n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 파일 형식입니다: {ext}\")\n",
    "\n",
    "def scale_bbox(bbox, current_size, target_size):\n",
    "    \"\"\"Bounding box 좌표를 현재 크기에서 목표 크기로 스케일링합니다.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    current_w, current_h = current_size\n",
    "    target_w, target_h = target_size\n",
    "    \n",
    "    if current_w == 0 or current_h == 0: return [0, 0, 0, 0]\n",
    "        \n",
    "    scale_x = target_w / current_w\n",
    "    scale_y = target_h / current_h\n",
    "    return [\n",
    "        int(x1 * scale_x), int(y1 * scale_y),\n",
    "        int(x2 * scale_x), int(y2 * scale_y)\n",
    "    ]\n",
    "\n",
    "def extract_text_with_vlm(image_pil, bbox, debug_info=None):\n",
    "    \"\"\"이미지의 특정 bbox 영역을 잘라 VLM으로 OCR을 수행합니다.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    img_w, img_h = image_pil.size\n",
    "    # bbox 좌표가 이미지 경계를 벗어나지 않도록 보정\n",
    "    x1, y1, x2, y2 = max(0, x1), max(0, y1), min(img_w, x2), min(img_h, y2)\n",
    "    \n",
    "    if x1 >= x2 or y1 >= y2: return \"\"\n",
    "        \n",
    "    cropped_image = image_pil.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # 너무 작은 이미지는 처리하지 않음\n",
    "    if cropped_image.width < 10 or cropped_image.height < 10: return \"\"\n",
    "    \n",
    "    # 크롭된 이미지 시각화 저장 (디버깅용)\n",
    "    if debug_info is not None:\n",
    "        debug_dir = \"./debug_crops\"\n",
    "        os.makedirs(debug_dir, exist_ok=True)\n",
    "        crop_filename = f\"{debug_info['id']}_{debug_info['category']}_{debug_info['order']}.png\"\n",
    "        crop_path = os.path.join(debug_dir, crop_filename)\n",
    "        cropped_image.save(crop_path)\n",
    "        print(f\"🔍 Cropped image saved: {crop_path} (size: {cropped_image.size})\")\n",
    "\n",
    "    # VLM이 로드되지 않았으면 pytesseract 사용\n",
    "    if ocr_processor is None or ocr_model is None:\n",
    "        try:\n",
    "            # pytesseract로 OCR 수행\n",
    "            text = pytesseract.image_to_string(cropped_image, lang='kor+eng').strip()\n",
    "            print(f\"🔤 Pytesseract OCR result: '{text}' (from {debug_info['id']}_{debug_info['category']}_{debug_info['order']})\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during pytesseract OCR: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    # 더 강력한 OCR 프롬프트로 수정\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": cropped_image,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Extract all text visible in this image. Return only the raw text without any explanations or apologies. If no text is visible, return empty.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Apply chat template\n",
    "        text = ocr_processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = ocr_processor(\n",
    "            text=[text],\n",
    "            images=[cropped_image],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        \n",
    "        # GPU 사용 시에만 cuda로 이동\n",
    "        if device.type == 'cuda':\n",
    "            inputs = inputs.to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            generated_ids = ocr_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=ocr_processor.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = ocr_processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during VLM OCR on a cropped image: {e}\")\n",
    "        # fallback to pytesseract\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(cropped_image, lang='kor+eng').strip()\n",
    "            print(f\"🔤 (fallback) Pytesseract OCR: '{text}'\")\n",
    "            return text\n",
    "        except Exception as e2:\n",
    "            print(f\"Error during pytesseract fallback: {e2}\")\n",
    "            return \"\"\n",
    "\n",
    "def visualize_predictions(image_pil, predictions, save_path):\n",
    "    \"\"\"예측 결과를 이미지에 시각화해서 저장합니다.\"\"\"\n",
    "    # 이미지 복사본 생성\n",
    "    vis_image = image_pil.copy()\n",
    "    draw = ImageDraw.Draw(vis_image)\n",
    "    \n",
    "    # 카테고리별 색상 정의\n",
    "    colors = {\n",
    "        'title': 'red',\n",
    "        'subtitle': 'orange', \n",
    "        'text': 'blue',\n",
    "        'equation': 'green',\n",
    "        'table': 'purple',\n",
    "        'image': 'pink'\n",
    "    }\n",
    "    \n",
    "    # 폰트 설정 (기본 폰트 사용)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    for pred in predictions:\n",
    "        bbox_str = pred['bbox']\n",
    "        x1, y1, x2, y2 = map(int, bbox_str.split(','))\n",
    "        category = pred['category_type']\n",
    "        confidence = pred['confidence_score']\n",
    "        order = pred['order']\n",
    "        \n",
    "        # 바운딩 박스 그리기\n",
    "        color = colors.get(category, 'gray')\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # 라벨 텍스트\n",
    "        label = f\"{category}_{order} ({confidence:.2f})\"\n",
    "        \n",
    "        # 텍스트 배경 박스\n",
    "        text_bbox = draw.textbbox((x1, y1-25), label, font=font)\n",
    "        draw.rectangle(text_bbox, fill=color)\n",
    "        draw.text((x1, y1-25), label, fill='white', font=font)\n",
    "    \n",
    "    # 저장\n",
    "    vis_image.save(save_path)\n",
    "    print(f\"📊 Visualization saved: {save_path}\")\n",
    "\n",
    "def inference_one_image(id_val, image_pil, target_size, conf_thres=0.15, imgsz=1920):\n",
    "    \"\"\"단일 이미지에 대해 레이아웃 감지 및 OCR 추론을 수행합니다.\"\"\"\n",
    "    original_size = image_pil.size\n",
    "    \n",
    "    # YOLO 추론을 위해 이미지 리사이즈 (YOLO 모델 학습 시 사용된 크기)\n",
    "    # 임시 파일 저장을 통해 메모리 문제를 완화할 수 있음\n",
    "    temp_path = \"_temp_image.png\"\n",
    "    image_pil.resize((imgsz, imgsz)).save(temp_path)\n",
    "\n",
    "    results = layout_model(\n",
    "        source=temp_path, \n",
    "        imgsz=imgsz, \n",
    "        conf=conf_thres, \n",
    "        iou=0.3,  # NMS IoU threshold - 낮을수록 더 많은 박스 유지\n",
    "        agnostic_nms=True,  # class-agnostic NMS\n",
    "        max_det=300,  # 최대 감지 수 증가\n",
    "        verbose=False\n",
    "    )[0]\n",
    "    os.remove(temp_path)\n",
    "\n",
    "    predictions = []\n",
    "    if results.boxes is None: return []\n",
    "\n",
    "    # Bbox를 y좌표 기준으로 정렬하여 문서의 위->아래 순서로 처리\n",
    "    sorted_boxes = sorted(zip(results.boxes.xyxy, results.boxes.conf, results.boxes.cls), key=lambda x: x[0][1])\n",
    "\n",
    "    for order, (box, score, cls) in enumerate(sorted_boxes):\n",
    "        label = results.names[int(cls)]\n",
    "        if label not in LABEL_MAP: continue\n",
    "            \n",
    "        category_type = LABEL_MAP[label]\n",
    "        \n",
    "        # 1. 추론된 bbox(imgsz x imgsz 기준)를 원본 이미지 크기로 변환\n",
    "        original_bbox = scale_bbox(box.tolist(), (imgsz, imgsz), original_size)\n",
    "        \n",
    "        text = ''\n",
    "        if category_type in ['title', 'subtitle', 'text']:\n",
    "            debug_info = {\n",
    "                'id': id_val,\n",
    "                'category': category_type, \n",
    "                'order': order\n",
    "            }\n",
    "            text = extract_text_with_vlm(image_pil, original_bbox, debug_info)\n",
    "        \n",
    "        # 2. 원본 기준 bbox를 최종 제출 형식(target_size)에 맞게 변환\n",
    "        final_bbox = scale_bbox(original_bbox, original_size, target_size)\n",
    "\n",
    "        predictions.append({\n",
    "            'ID': id_val,\n",
    "            'category_type': category_type,\n",
    "            'confidence_score': score.cpu().item(),\n",
    "            'order': order,\n",
    "            'text': text,\n",
    "            'bbox': f'{final_bbox[0]},{final_bbox[1]},{final_bbox[2]},{final_bbox[3]}'\n",
    "        })\n",
    "    \n",
    "    # 시각화 저장 (디버깅용)\n",
    "    debug_vis_dir = \"./debug_visualizations\"\n",
    "    os.makedirs(debug_vis_dir, exist_ok=True)\n",
    "    vis_filename = f\"{id_val}_visualization.png\"\n",
    "    vis_path = os.path.join(debug_vis_dir, vis_filename)\n",
    "    \n",
    "    # 원본 이미지 크기로 bbox 변환해서 시각화\n",
    "    vis_predictions = []\n",
    "    for pred in predictions:\n",
    "        vis_pred = pred.copy()\n",
    "        # target_size 기준 bbox를 원본 크기로 다시 변환\n",
    "        bbox_coords = list(map(int, pred['bbox'].split(',')))\n",
    "        original_bbox_coords = scale_bbox(bbox_coords, target_size, original_size)\n",
    "        vis_pred['bbox'] = f'{original_bbox_coords[0]},{original_bbox_coords[1]},{original_bbox_coords[2]},{original_bbox_coords[3]}'\n",
    "        vis_predictions.append(vis_pred)\n",
    "    \n",
    "    visualize_predictions(image_pil, vis_predictions, vis_path)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def main_inference(test_csv_path, output_csv_path, conf_thres=0.15, imgsz=1920):\n",
    "    \"\"\"메인 추론 함수: CSV를 읽고, 모든 파일에 대해 추론을 실행하며, 결과를 저장합니다.\"\"\"\n",
    "    output_dir = os.path.dirname(output_csv_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_image_dir = \"./temp_images\"\n",
    "    os.makedirs(temp_image_dir, exist_ok=True)\n",
    "    \n",
    "    csv_dir = os.path.dirname(test_csv_path)\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    all_preds = []\n",
    "\n",
    "    print(f\"Using confidence threshold: {conf_thres}, image size: {imgsz}\")\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        id_val = row['ID']\n",
    "        raw_path = row['path']\n",
    "        file_path = os.path.normpath(os.path.join(csv_dir, raw_path))\n",
    "        target_size = (int(row['width']), int(row['height']))\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            images = convert_to_images(file_path, temp_image_dir)\n",
    "            for i, image in enumerate(images):\n",
    "                # 멀티페이지 문서 ID 형식 (예: doc1_p1, doc1_p2)\n",
    "                full_id = f\"{id_val}_p{i+1}\" if len(images) > 1 else id_val\n",
    "                preds = inference_one_image(full_id, image, target_size, conf_thres=conf_thres, imgsz=imgsz)\n",
    "                all_preds.extend(preds)\n",
    "            print(f\"✅ Inference complete: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Processing failed: {file_path} -> {e}\")\n",
    "\n",
    "    result_df = pd.DataFrame(all_preds)\n",
    "    result_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ Submission file saved: {output_csv_path}\")\n",
    "\n",
    "\n",
    "# --- 스크립트 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 대회 규정에 맞는 경로 설정\n",
    "    # test.csv는 보통 /data/test.csv 에 위치합니다.\n",
    "    # 제출 파일은 /output/submission.csv 에 저장해야 합니다.\n",
    "    data_dir = os.environ.get('DATA_DIR', './data')\n",
    "    output_dir = os.environ.get('OUTPUT_DIR', './output')\n",
    "    \n",
    "    # 환경 변수가 없는 로컬 테스트를 위해 폴더 생성\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    test_csv_file = os.path.join(data_dir, 'test.csv')\n",
    "    submission_file = os.path.join(output_dir, 'submission.csv')\n",
    "\n",
    "    # 성능 튜닝을 위한 파라미터\n",
    "    # conf_thres: 0.15 (낮음) ~ 0.5 (높음) - 낮을수록 더 많은 박스 검출\n",
    "    # imgsz: 1280, 1536, 1920 - 클수록 정확도 향상, 속도 감소\n",
    "    conf_threshold = 0.15  # 더 낮춰서 더 많은 박스 검출\n",
    "    image_size = 1920     # 더 크게 해서 정확도 향상\n",
    "    \n",
    "    print(f\"Performance settings: conf_thres={conf_threshold}, imgsz={image_size}\")\n",
    "    main_inference(\n",
    "        test_csv_path=test_csv_file, \n",
    "        output_csv_path=submission_file,\n",
    "        conf_thres=conf_threshold,\n",
    "        imgsz=image_size\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
