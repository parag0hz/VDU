{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98657c42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ultralytics import YOLO\n",
    "from pdf2image import convert_from_path\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import pytesseract\n",
    "\n",
    "# --- í™˜ê²½ ì„¤ì • ---\n",
    "# GPU ì‚¬ìš© ì„¤ì • (í‰ê°€ ì„œë²„ì— GPUê°€ ìˆìœ¼ë¯€ë¡œ 'cuda'ë¡œ ì„¤ì •ë©ë‹ˆë‹¤)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- ëª¨ë¸ ë¡œë”© (ì „ë¶€ ë¡œì»¬ ê²½ë¡œì—ì„œ ë¡œë“œ) ---\n",
    "\n",
    "# 1. YOLO ë ˆì´ì•„ì›ƒ ëª¨ë¸ ë¡œë”© (ì œì¶œ í´ë” ë‚´ 'model' í´ë”)\n",
    "layout_model_path = \"./model/yolov12l-doclaynet.pt\"\n",
    "layout_model = YOLO(layout_model_path)\n",
    "print(f\"Layout model loaded from: {layout_model_path}\")\n",
    "\n",
    "# 2. Qwen2-VL OCR ëª¨ë¸ ë¡œë”© (ì œì¶œ í´ë” ë‚´ 'models' í´ë”)\n",
    "# ì´ ê²½ë¡œëŠ” submit.zip íŒŒì¼ ë‚´ë¶€ êµ¬ì¡°ì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "local_vlm_path = \"./model/qwen2.5-vl-3b-instruct\"\n",
    "print(f\"Loading VLM-OCR model from local files: {local_vlm_path}...\")\n",
    "\n",
    "try:\n",
    "    # ë¨¼ì € processorë§Œ ë¡œë”© ì‹œë„\n",
    "    print(\"Loading processor...\")\n",
    "    ocr_processor = AutoProcessor.from_pretrained(local_vlm_path, trust_remote_code=True)\n",
    "    print(\"Processor loaded successfully.\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë”© ì‹œë„ - AutoModelForImageTextToText ì‚¬ìš©\n",
    "    print(\"Loading model...\")\n",
    "    \n",
    "    # Qwen2-VL-2B-Instruct ëª¨ë¸ ë¡œë”©\n",
    "    ocr_model = AutoModelForImageTextToText.from_pretrained(\n",
    "        local_vlm_path, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # GPUë¡œ ì´ë™\n",
    "    if device.type == 'cuda':\n",
    "        ocr_model = ocr_model.to(device)\n",
    "        \n",
    "    print(\"VLM-OCR model loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading VLM-OCR model: {e}\")\n",
    "    print(\"Falling back to basic OCR...\")\n",
    "    ocr_processor = None\n",
    "    ocr_model = None\n",
    "\n",
    "# --- ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì • ---\n",
    "# í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘\n",
    "LABEL_MAP = {\n",
    "    'Text': 'text',\n",
    "    'Title': 'title',\n",
    "    'Section-header': 'subtitle',\n",
    "    'Formula': 'equation',\n",
    "    'Table': 'table',\n",
    "    'Picture': 'image'\n",
    "}\n",
    "\n",
    "# --- í•¨ìˆ˜ ì •ì˜ ---\n",
    "\n",
    "def convert_to_images(input_path, temp_dir, dpi=200):\n",
    "    ext = Path(input_path).suffix.lower()\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        return convert_from_path(input_path, dpi=dpi, output_folder=temp_dir, fmt=\"png\")\n",
    "    elif ext == \".pptx\":\n",
    "        # Convert pptx to pdf first\n",
    "        subprocess.run([\n",
    "            \"libreoffice\", \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", temp_dir, input_path\n",
    "        ], check=True)\n",
    "        pdf_path = os.path.join(temp_dir, Path(input_path).with_suffix(\".pdf\").name)\n",
    "        return convert_from_path(pdf_path, dpi=dpi, output_folder=temp_dir, fmt=\"png\")\n",
    "    elif ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        return [Image.open(input_path).convert(\"RGB\")]\n",
    "    else:\n",
    "        raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}\")\n",
    "\n",
    "def scale_bbox(bbox, current_size, target_size):\n",
    "    \"\"\"Bounding box ì¢Œí‘œë¥¼ í˜„ì¬ í¬ê¸°ì—ì„œ ëª©í‘œ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    current_w, current_h = current_size\n",
    "    target_w, target_h = target_size\n",
    "    \n",
    "    if current_w == 0 or current_h == 0: return [0, 0, 0, 0]\n",
    "        \n",
    "    scale_x = target_w / current_w\n",
    "    scale_y = target_h / current_h\n",
    "    return [\n",
    "        int(x1 * scale_x), int(y1 * scale_y),\n",
    "        int(x2 * scale_x), int(y2 * scale_y)\n",
    "    ]\n",
    "\n",
    "def extract_text_with_vlm(image_pil, bbox, debug_info=None):\n",
    "    \"\"\"ì´ë¯¸ì§€ì˜ íŠ¹ì • bbox ì˜ì—­ì„ ì˜ë¼ VLMìœ¼ë¡œ OCRì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    img_w, img_h = image_pil.size\n",
    "    # bbox ì¢Œí‘œê°€ ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ë³´ì •\n",
    "    x1, y1, x2, y2 = max(0, x1), max(0, y1), min(img_w, x2), min(img_h, y2)\n",
    "    \n",
    "    if x1 >= x2 or y1 >= y2: return \"\"\n",
    "        \n",
    "    cropped_image = image_pil.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ\n",
    "    if cropped_image.width < 10 or cropped_image.height < 10: return \"\"\n",
    "    \n",
    "    # í¬ë¡­ëœ ì´ë¯¸ì§€ ì‹œê°í™” ì €ì¥ (ë””ë²„ê¹…ìš©)\n",
    "    if debug_info is not None:\n",
    "        debug_dir = \"./debug_crops\"\n",
    "        os.makedirs(debug_dir, exist_ok=True)\n",
    "        crop_filename = f\"{debug_info['id']}_{debug_info['category']}_{debug_info['order']}.png\"\n",
    "        crop_path = os.path.join(debug_dir, crop_filename)\n",
    "        cropped_image.save(crop_path)\n",
    "        print(f\"ğŸ” Cropped image saved: {crop_path} (size: {cropped_image.size})\")\n",
    "\n",
    "    # VLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ pytesseract ì‚¬ìš©\n",
    "    if ocr_processor is None or ocr_model is None:\n",
    "        try:\n",
    "            # pytesseractë¡œ OCR ìˆ˜í–‰\n",
    "            text = pytesseract.image_to_string(cropped_image, lang='kor+eng').strip()\n",
    "            print(f\"ğŸ”¤ Pytesseract OCR result: '{text}' (from {debug_info['id']}_{debug_info['category']}_{debug_info['order']})\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error during pytesseract OCR: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    # ë” ê°•ë ¥í•œ OCR í”„ë¡¬í”„íŠ¸ë¡œ ìˆ˜ì •\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": cropped_image,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Extract all text visible in this image. Return only the raw text without any explanations or apologies. If no text is visible, return empty.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Apply chat template\n",
    "        text = ocr_processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = ocr_processor(\n",
    "            text=[text],\n",
    "            images=[cropped_image],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        \n",
    "        # GPU ì‚¬ìš© ì‹œì—ë§Œ cudaë¡œ ì´ë™\n",
    "        if device.type == 'cuda':\n",
    "            inputs = inputs.to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            generated_ids = ocr_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=ocr_processor.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = ocr_processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during VLM OCR on a cropped image: {e}\")\n",
    "        # fallback to pytesseract\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(cropped_image, lang='kor+eng').strip()\n",
    "            print(f\"ğŸ”¤ (fallback) Pytesseract OCR: '{text}'\")\n",
    "            return text\n",
    "        except Exception as e2:\n",
    "            print(f\"Error during pytesseract fallback: {e2}\")\n",
    "            return \"\"\n",
    "\n",
    "def visualize_predictions(image_pil, predictions, save_path):\n",
    "    \"\"\"ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ì‹œê°í™”í•´ì„œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # ì´ë¯¸ì§€ ë³µì‚¬ë³¸ ìƒì„±\n",
    "    vis_image = image_pil.copy()\n",
    "    draw = ImageDraw.Draw(vis_image)\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ ì •ì˜\n",
    "    colors = {\n",
    "        'title': 'red',\n",
    "        'subtitle': 'orange', \n",
    "        'text': 'blue',\n",
    "        'equation': 'green',\n",
    "        'table': 'purple',\n",
    "        'image': 'pink'\n",
    "    }\n",
    "    \n",
    "    # í°íŠ¸ ì„¤ì • (ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    for pred in predictions:\n",
    "        bbox_str = pred['bbox']\n",
    "        x1, y1, x2, y2 = map(int, bbox_str.split(','))\n",
    "        category = pred['category_type']\n",
    "        confidence = pred['confidence_score']\n",
    "        order = pred['order']\n",
    "        \n",
    "        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
    "        color = colors.get(category, 'gray')\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # ë¼ë²¨ í…ìŠ¤íŠ¸\n",
    "        label = f\"{category}_{order} ({confidence:.2f})\"\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë°°ê²½ ë°•ìŠ¤\n",
    "        text_bbox = draw.textbbox((x1, y1-25), label, font=font)\n",
    "        draw.rectangle(text_bbox, fill=color)\n",
    "        draw.text((x1, y1-25), label, fill='white', font=font)\n",
    "    \n",
    "    # ì €ì¥\n",
    "    vis_image.save(save_path)\n",
    "    print(f\"ğŸ“Š Visualization saved: {save_path}\")\n",
    "\n",
    "def inference_one_image(id_val, image_pil, target_size, conf_thres=0.15, imgsz=1920):\n",
    "    \"\"\"ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•´ ë ˆì´ì•„ì›ƒ ê°ì§€ ë° OCR ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    original_size = image_pil.size\n",
    "    \n",
    "    # YOLO ì¶”ë¡ ì„ ìœ„í•´ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (YOLO ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ í¬ê¸°)\n",
    "    # ì„ì‹œ íŒŒì¼ ì €ì¥ì„ í†µí•´ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆìŒ\n",
    "    temp_path = \"_temp_image.png\"\n",
    "    image_pil.resize((imgsz, imgsz)).save(temp_path)\n",
    "\n",
    "    results = layout_model(\n",
    "        source=temp_path, \n",
    "        imgsz=imgsz, \n",
    "        conf=conf_thres, \n",
    "        iou=0.3,  # NMS IoU threshold - ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ ë°•ìŠ¤ ìœ ì§€\n",
    "        agnostic_nms=True,  # class-agnostic NMS\n",
    "        max_det=300,  # ìµœëŒ€ ê°ì§€ ìˆ˜ ì¦ê°€\n",
    "        verbose=False\n",
    "    )[0]\n",
    "    os.remove(temp_path)\n",
    "\n",
    "    predictions = []\n",
    "    if results.boxes is None: return []\n",
    "\n",
    "    # Bboxë¥¼ yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë¬¸ì„œì˜ ìœ„->ì•„ë˜ ìˆœì„œë¡œ ì²˜ë¦¬\n",
    "    sorted_boxes = sorted(zip(results.boxes.xyxy, results.boxes.conf, results.boxes.cls), key=lambda x: x[0][1])\n",
    "\n",
    "    for order, (box, score, cls) in enumerate(sorted_boxes):\n",
    "        label = results.names[int(cls)]\n",
    "        if label not in LABEL_MAP: continue\n",
    "            \n",
    "        category_type = LABEL_MAP[label]\n",
    "        \n",
    "        # 1. ì¶”ë¡ ëœ bbox(imgsz x imgsz ê¸°ì¤€)ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³€í™˜\n",
    "        original_bbox = scale_bbox(box.tolist(), (imgsz, imgsz), original_size)\n",
    "        \n",
    "        text = ''\n",
    "        if category_type in ['title', 'subtitle', 'text']:\n",
    "            debug_info = {\n",
    "                'id': id_val,\n",
    "                'category': category_type, \n",
    "                'order': order\n",
    "            }\n",
    "            text = extract_text_with_vlm(image_pil, original_bbox, debug_info)\n",
    "        \n",
    "        # 2. ì›ë³¸ ê¸°ì¤€ bboxë¥¼ ìµœì¢… ì œì¶œ í˜•ì‹(target_size)ì— ë§ê²Œ ë³€í™˜\n",
    "        final_bbox = scale_bbox(original_bbox, original_size, target_size)\n",
    "\n",
    "        predictions.append({\n",
    "            'ID': id_val,\n",
    "            'category_type': category_type,\n",
    "            'confidence_score': score.cpu().item(),\n",
    "            'order': order,\n",
    "            'text': text,\n",
    "            'bbox': f'{final_bbox[0]},{final_bbox[1]},{final_bbox[2]},{final_bbox[3]}'\n",
    "        })\n",
    "    \n",
    "    # ì‹œê°í™” ì €ì¥ (ë””ë²„ê¹…ìš©)\n",
    "    debug_vis_dir = \"./debug_visualizations\"\n",
    "    os.makedirs(debug_vis_dir, exist_ok=True)\n",
    "    vis_filename = f\"{id_val}_visualization.png\"\n",
    "    vis_path = os.path.join(debug_vis_dir, vis_filename)\n",
    "    \n",
    "    # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ bbox ë³€í™˜í•´ì„œ ì‹œê°í™”\n",
    "    vis_predictions = []\n",
    "    for pred in predictions:\n",
    "        vis_pred = pred.copy()\n",
    "        # target_size ê¸°ì¤€ bboxë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë‹¤ì‹œ ë³€í™˜\n",
    "        bbox_coords = list(map(int, pred['bbox'].split(',')))\n",
    "        original_bbox_coords = scale_bbox(bbox_coords, target_size, original_size)\n",
    "        vis_pred['bbox'] = f'{original_bbox_coords[0]},{original_bbox_coords[1]},{original_bbox_coords[2]},{original_bbox_coords[3]}'\n",
    "        vis_predictions.append(vis_pred)\n",
    "    \n",
    "    visualize_predictions(image_pil, vis_predictions, vis_path)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def main_inference(test_csv_path, output_csv_path, conf_thres=0.15, imgsz=1920):\n",
    "    \"\"\"ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜: CSVë¥¼ ì½ê³ , ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ë©°, ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    output_dir = os.path.dirname(output_csv_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_image_dir = \"./temp_images\"\n",
    "    os.makedirs(temp_image_dir, exist_ok=True)\n",
    "    \n",
    "    csv_dir = os.path.dirname(test_csv_path)\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    all_preds = []\n",
    "\n",
    "    print(f\"Using confidence threshold: {conf_thres}, image size: {imgsz}\")\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        id_val = row['ID']\n",
    "        raw_path = row['path']\n",
    "        file_path = os.path.normpath(os.path.join(csv_dir, raw_path))\n",
    "        target_size = (int(row['width']), int(row['height']))\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"âš ï¸ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            images = convert_to_images(file_path, temp_image_dir)\n",
    "            for i, image in enumerate(images):\n",
    "                # ë©€í‹°í˜ì´ì§€ ë¬¸ì„œ ID í˜•ì‹ (ì˜ˆ: doc1_p1, doc1_p2)\n",
    "                full_id = f\"{id_val}_p{i+1}\" if len(images) > 1 else id_val\n",
    "                preds = inference_one_image(full_id, image, target_size, conf_thres=conf_thres, imgsz=imgsz)\n",
    "                all_preds.extend(preds)\n",
    "            print(f\"âœ… Inference complete: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Processing failed: {file_path} -> {e}\")\n",
    "\n",
    "    result_df = pd.DataFrame(all_preds)\n",
    "    result_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… Submission file saved: {output_csv_path}\")\n",
    "\n",
    "\n",
    "# --- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ëŒ€íšŒ ê·œì •ì— ë§ëŠ” ê²½ë¡œ ì„¤ì •\n",
    "    # test.csvëŠ” ë³´í†µ /data/test.csv ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.\n",
    "    # ì œì¶œ íŒŒì¼ì€ /output/submission.csv ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    data_dir = os.environ.get('DATA_DIR', './data')\n",
    "    output_dir = os.environ.get('OUTPUT_DIR', './output')\n",
    "    \n",
    "    # í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ëŠ” ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í´ë” ìƒì„±\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    test_csv_file = os.path.join(data_dir, 'test.csv')\n",
    "    submission_file = os.path.join(output_dir, 'submission.csv')\n",
    "\n",
    "    # ì„±ëŠ¥ íŠœë‹ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°\n",
    "    # conf_thres: 0.15 (ë‚®ìŒ) ~ 0.5 (ë†’ìŒ) - ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ ë°•ìŠ¤ ê²€ì¶œ\n",
    "    # imgsz: 1280, 1536, 1920 - í´ìˆ˜ë¡ ì •í™•ë„ í–¥ìƒ, ì†ë„ ê°ì†Œ\n",
    "    conf_threshold = 0.15  # ë” ë‚®ì¶°ì„œ ë” ë§ì€ ë°•ìŠ¤ ê²€ì¶œ\n",
    "    image_size = 1920     # ë” í¬ê²Œ í•´ì„œ ì •í™•ë„ í–¥ìƒ\n",
    "    \n",
    "    print(f\"Performance settings: conf_thres={conf_threshold}, imgsz={image_size}\")\n",
    "    main_inference(\n",
    "        test_csv_path=test_csv_file, \n",
    "        output_csv_path=submission_file,\n",
    "        conf_thres=conf_threshold,\n",
    "        imgsz=image_size\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
